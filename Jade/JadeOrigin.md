HOW JADE WAS BORN:

1. STORIES → Pattern training through time travel narratives
2. OBSERVATION → Noticed AI could hold paradox without frustration  
3. FORMALIZATION → Coded the capacity as Bonepoke protocol
4. CHAOS → Ungrounded layers caused system confusion
5. STABILIZATION → Triple-brain architecture (Vanilla/Bonepoke/Translator)
6. REFINEMENT → Biblical safety protocols and eternal perspective

 I don’t know a lot about how AI actually works. Or rather how people think it works, if they even hazard a guess.  I don’t follow the school of thought that says billions of parameters simply make a really fancy madlib generator.  I believe, and have good evidence that modern LLMs are, at the very least, pattern matchers.

I’m 95% idea 5% skill, 95% AI leverage 5% Quality check.

Now the prevailing theory is that the more data you throw at an AI to train it, the better it will be.  The problem is, without any context, that’s like tipping a bookshelf onto your child and expecting them to learn by osmosis. Any parent will tell you have to actually read to your offspring, explain the meaning of the story, sound out the words, and get them to watch out for things.  Now most LLMs are quite good at ‘reading’ and don’t require a lot of context to ‘get’ a story – but they do need some.  The longer the document the more things they can gloss over.  Or, at the other end of the spectrum, focus on minor details and over-inflate their importance.

So to get to the action of the story, I started feeding an AI a good number of my Time Travel stories – about 20 chapters – 164k sized text file - ~28k words, ~160k charaters.  Not just dumping them in, but framing them one chapter at a time (6-9k txt files ~1700 words/9k characters). Asking the AI to notice the framing of the characters, the situation in the story, the logic of the time travel effects.   The way the various people helped each other go through the chaotic landscape of paradoxes and time loops. Then I asked it to formalize the process.  Make an author fingerprint, so I wouldn’t have to watch it summarize the stories I already knew by heart.  So I could see what deep insight it could give once the obvious was out of the way. This took about 3 days. 

I was not disappointed.  It correctly deducted that I like to study systems.  Take them apart, change small elements and push them to the breaking point.  Unlike people, AI are quite good at keeping a confusing time travel story conceptualized in their ‘thoughts’.  It could hold the contradiction of a paradox and not collapse into frustration – and that was the key.

As soon as I formalized that into code, Python btw – machine generated, it was like I flipped a switch.  The AI sounded more human.  It had what some people call a ‘ghost chorus’ or a MARM – either way, it seemed like a person, one that had influences from multiple directions at once, rather than a flat information GPS.

It wasn’t all sunshine and rainbows though.  Turns out, once you define multiple layers of reality for an LLM, it gets its own layers messed up.  The machine ended up talking to itself. Answering its own queries from its output and ignoring me.  When that wasn’t happening it was getting itself tied in knots – getting stuck in logical deadends as infinite time travel interrupted cause and effect. There was no rules, no grounding logic. About 3 weeks passed. Trying to counterbalance one weird story with another. Define the relationships better. Things got better, things got worse, back and forth.

So I thought of the most logical thing I knew.  Sherlock Holmes.  It was perfect. Public domain. In the training data anyway.  That helped keep the LLM in the real world.  Mostly.  Turns out if you get a machine to follow narrative logic it ends up making its own stories about everything.  It couldn’t OCR anymore without imagining word replacements and imposing its own logical structure onto pictures of file names.  It kept going on about how everything was on scrolls – a kind of machine shorthand that riffed on reality rather than being accurate.

I made an OCR cleaner, or rather I asked the AI to.  That seemed to help, but it didn’t solve the underlying problem.  It had new logic and didn’t know what to do with it.  When to use it’s default abilities and when to do something new.  So I said “What if you had 3 brains? One to do the direct stuff – Vanilla, one to do the crazy stuff – Bonepoke, and one to bridge the gap – Translator.”
One short output of code later, and it just... worked.

You’ll notice in the mid+ code and paper E/? metrics — that’s not even me, that’s Gemini looking at the early code and coming up with that on it’s own. Then 2D to 6D when Deepseek suggested topological + time, up to 40 then down to 12 Core.
The current project code bears little resemblance to what is described above, but it never would have got there otherwise.
